# -*- coding: utf-8 -*-
"""Modified RealNVP New Layers 5D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mx_cXIESXadpgDICoYng3d_k9E9asgVg
"""

import torch
from torch import nn
from torch import distributions
import time
from torch.utils.data import DataLoader

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline


from sklearn import datasets

class RealNVP(nn.Module):
    def __init__(self, net_s, net_t, masks, prior):
        super(RealNVP, self).__init__()
        self.prior = prior
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # enable gpu processing
        self.masks = nn.Parameter(masks, requires_grad=False).to(self.device)
        self.t = torch.nn.ModuleList([net_t() for _ in range(len(masks))]).to(self.device)
        self.s = torch.nn.ModuleList([net_s() for _ in range(len(masks))]).to(self.device)

        self.vec_dim = len(masks)

    def g(self, z):  # inverse mapping (latent space to primary space)
        x = z
        for i in reversed(range(len(self.t))):
            x_ = x * self.masks[i]
            s = self.s[i](x_) * (1 - self.masks[i])
            t = self.t[i](x_) * (1 - self.masks[i])
            x = x_ + (1 - masks[i]) * ((x - t) * torch.exp(-s))
        return x  # sample in primary space

    def f(self, x):  # forward mapping from primary space to latent space
        y = x
        log_det_J = x.new_zeros(x.shape[0])  # create array of zeros with the number of rows of x
        for i in range(len(self.t)):
            y_ = self.masks[i] * y  # apply mask i to input vector, preventing these parameters being sent to NN
            s = self.s[i](y_) * (
                        1 - masks[i])  # compute the scale factor for this layer and select only the active part

            t = self.t[i](y_) * (1 - masks[i])  # compute the translation factor
            y = y_ + (1 - self.masks[i]) * (y * torch.exp(
                s) + t)  # apply exp scale and translation to input vector and selected unmasked parts
            log_det_J += s.sum(dim=1)

        return y, log_det_J  # y is in latent space, log_det_J is the log of the Jacobian of the transformation

    def log_prob(self, x):
        z, logp = self.f(x)
        return logp + self.prior.log_prob(z)

    def sample(self, batchSize=1):
        z = self.prior.sample((batchSize,))  # draw sample in latent space
        logp = self.prior.log_prob(z)
        print(z.shape)
        x = self.g(z)
        return x, z



if __name__ == '__main__':

    # main contains some code for testing out and debugging

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # enable gpu processing
    prior = distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))

    # to tweak
    masks = torch.from_numpy(np.array([[0, 1], [1, 0]] * 4).astype(np.float32)).to(
        device)  # need to do this here for it to work
    # masks = torch.from_numpy(np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1]] * 3).astype(np.float32)).to(device)
    num_neurons = 40
    in_dim = 2  # in dim and out dim are the same


    # nets = lambda: nn.Sequential(nn.BatchNorm1d(in_dim), nn.Linear(in_dim, num_neurons), nn.BatchNorm1d(num_neurons),
    #                              nn.ReLU(),  # input block
    #                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(),
    #                              # input block
    #                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(),
    #                              # residual block
    #                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(),
    #                              # residual block
    #                              nn.BatchNorm1d(num_neurons), nn.ReLU(), nn.Linear(num_neurons, in_dim), nn.Tanh()
    #                              # end block
    #                              )

    # nett = lambda: nn.Sequential(nn.BatchNorm1d(in_dim), nn.Linear(in_dim, num_neurons), nn.BatchNorm1d(num_neurons),
    #                              nn.ReLU(),  # input block
    #                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(),
    #                              # input block
    #                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(),
    #                              # residual block
    #                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(),
    #                              # residual block
    #                              nn.BatchNorm1d(num_neurons), nn.ReLU(), nn.Linear(num_neurons, in_dim)  # end block
    #                              )

    nets = lambda: nn.Sequential(nn.Linear(2, 200), nn.LeakyReLU(), nn.Linear(200, 200), nn.BatchNorm1d(200), nn.LeakyReLU(), nn.Linear(200, 2), nn.Tanh())
    nett = lambda: nn.Sequential(nn.Linear(2, 200), nn.LeakyReLU(), nn.Linear(200, 200), nn.BatchNorm1d(200), nn.LeakyReLU(), nn.Linear(200, 2))


    net = RealNVP(nets, nett, masks, prior)
    net = net.to(device)

    # create the data set and the data loaders
    # 10 000 val training set
    data_set_size = int(1e5)
    noisy_moons = datasets.make_moons(n_samples=data_set_size, noise=.05)[0].astype(np.float32)
    noisy_moons_tensor = torch.from_numpy(noisy_moons)

    train_dataloader = DataLoader(noisy_moons_tensor, batch_size=100, shuffle=True, num_workers=4)

    optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad == True], lr=1e-4)
    print(f"Running on: {device}")
    net.train()
    epoch_count = 5
    start_time = time.time()
    for t in range(epoch_count):
        print(f"Starting epoch {t}...")
        batch_cnt=0
        for batch in train_dataloader:
            optimizer.zero_grad()
            loss = -net.log_prob(batch.to(device)).mean()
            loss.backward(retain_graph=True)
            optimizer.step()

            batch_cnt+=1
            if batch_cnt % 100 == 0:
                print(f"Batch number: {batch_cnt}")

        print('iter %s:' % t, 'loss = %.3f' % loss)

    # freeze weights
    net.eval()
    print(f"Training time: {time.time()-start_time} seconds")

    # draw samples in primary space and map to latent space
    plt.subplot(121)
    x = datasets.make_moons(n_samples=1000, noise=.05)[0].astype(np.float32)
    plt.scatter(x[:, 0], x[:, 1], c='r')
    plt.title(r'$X \sim p(X)$')

    z = net.f(torch.from_numpy(x).to(device))[0].cpu().detach().numpy()

    plt.subplot(122)
    plt.scatter(z[:, 0], z[:, 1])
    plt.title(r'$z = f(X)$')

    # draw samples in latent space and map to primary space
    x, z = net.sample(1000)
    x = x.cpu().detach().numpy()
    z = z.cpu().detach().squeeze().numpy()

       # z = np.random.uniform([-.5, -.5], [.5,.5], (1000, 2))
    plt.subplot(121)
    plt.scatter(z[:, 0], z[:, 1])
    plt.title(r'$z \sim p(z)$')

    plt.subplot(122)

    plt.scatter(x[:, 0], x[:, 1], c='r')
    plt.title(r'$X = g(z)$')


    plt.show()