# -*- coding: utf-8 -*-
"""Modified RealNVP New Layers 5D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mx_cXIESXadpgDICoYng3d_k9E9asgVg
"""

import torch
from torch import nn
from torch.nn.parameter import Parameter
from torch import distributions

from torch.utils.data import DataLoader

import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline


from sklearn import datasets 
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") #enable gpu processing

class RealNVP(nn.Module):
  def __init__(self, net_s, net_t, masks, prior):
    super(RealNVP,self).__init__()
    self.prior = prior
    self.masks = nn.Parameter(masks, requires_grad=False).to(device)
    self.t = torch.nn.ModuleList([net_t() for _ in range(len(masks))]).to(device)
    self.s = torch.nn.ModuleList([net_s() for _ in range(len(masks))]).to(device)
    self.vec_dim = len(masks)
    
   
  def g(self, z): # inverse mapping (latent space to primary space)
    x = z  
    for i in reversed(range(len(self.t))):
      x_ = x * self.masks[i]
      s = self.s[i](x_) * (1-self.masks[i])
      t = self.t[i](x_) * (1- self.masks[i])
      x = x_ + (1-masks[i])*((x-t)*torch.exp(-s))         
    return x # sample in primary space            
       
  def f(self, x): # forward mapping from primary space to latent space    
    y = x
    log_det_J = x.new_zeros(x.shape[0]) # create array of zeros with the number of rows of x
    for i in range(len(self.t)):     
      y_ = self.masks[i] * y # apply mask i to input vector, preventing these parameters being sent to NN
      s = self.s[i](y_)* (1 - masks[i]) # compute the scale factor for this layer and select only the active part
      
      t = self.t[i](y_) * (1 - masks[i]) # compute the translation factor
      y = y_ + (1 - self.masks[i]) * (y * torch.exp(s) + t) # apply exp scale and translation to input vector and selected unmasked parts
      log_det_J += s.sum(dim=1)
      
    return y, log_det_J # y is in latent space, log_det_J is the log of the Jacobian of the transformation
    
    
  def log_prob(self, x):
    z, logp = self.f(x)       
    # p_z = self.prior.log_prob(z)
    # return  p_z + logp -> log(p_z)=0 if dist is uniform over [-.5,.5]  
  
    return  logp + self.prior.log_prob(z) #- 7 * (z[:,0]*z[:,0] + z[:,1]*z[:,1]) # add punishment factor for having this too large, 
                            # in order to punish values out side of the prob range for both components
                           # this is why a gaussian prior is nicer, it has full support
    
  def sample(self, batchSize):
    z = self.prior.sample((batchSize,)) # draw sample in latent space      
    logp = self.prior.log_prob(z)
    print(z.shape)
    x = self.g(z) 
    return x, z

# Uniform prior, with pairs of points in [-.5, .5]
#prior = torch.distributions.uniform.Uniform(torch.tensor([-.5,-.5]).to(device), torch.tensor([.5,.5]).to(device)) # 2D vectors

# debug and test stuff
# sample = prior.sample((3,1))
# print(sample)
# print(sample.shape)
# print(torch.squeeze(sample).shape)
# print(f"log prob: {prior.log_prob(sample)}")
# s2 =torch.tensor([1.0,1.0]).to(device) # log prob of impossible value is inf
# print(s2)
# print(f"log prob: {prior.log_prob(s2)}")

# Gaussian prior
prior = distributions.MultivariateNormal(torch.zeros(2).to(device), torch.eye(2).to(device))

# to tweak
masks = torch.from_numpy(np.array([[0, 1], [1, 0]] * 4).astype(np.float32)).to(device) # need to do this here for it to work
#masks = torch.from_numpy(np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1]] * 3).astype(np.float32)).to(device) 
num_neurons = 40
in_dim = 2 # in dim and out dim are the same


# these will get passed by reference and not work properly
residual_block = [nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), 
                                       nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU()]

start_block =[nn.BatchNorm1d(in_dim), nn.Linear(in_dim,num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU()]

end_block = [nn.BatchNorm1d(num_neurons), nn.ReLU(), nn.Linear(num_neurons, in_dim), nn.Tanh()]

nets =  lambda: nn.Sequential(nn.BatchNorm1d(in_dim), nn.Linear(in_dim,num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # input block
                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # input block 
                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # residual block
                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # residual block
                              nn.BatchNorm1d(num_neurons), nn.ReLU(), nn.Linear(num_neurons, in_dim), nn.Tanh() # end block
                             )

nett =  lambda: nn.Sequential(nn.BatchNorm1d(in_dim), nn.Linear(in_dim,num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # input block
                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # input block 
                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # residual block 
                              nn.Linear(num_neurons, num_neurons), nn.BatchNorm1d(num_neurons), nn.ReLU(), # residual block
                              nn.BatchNorm1d(num_neurons), nn.ReLU(), nn.Linear(num_neurons, in_dim) # end block
                             )



# nets = lambda: nn.Sequential(nn.Linear(2, 200), nn.LeakyReLU(), nn.Linear(200, 200), nn.BatchNorm1d(200), nn.LeakyReLU(), nn.Linear(200, 2), nn.Tanh())
# nett = lambda: nn.Sequential(nn.Linear(2, 200), nn.LeakyReLU(), nn.Linear(200, 200), nn.BatchNorm1d(200), nn.LeakyReLU(), nn.Linear(200, 2))


net = RealNVP(nets, nett, masks, prior)
net = net.to(device)

# create the data set and the data loaders
# 10 000 val training set
data_set_size = int(1e5)
noisy_moons = datasets.make_moons(n_samples=data_set_size, noise=.05)[0].astype(np.float32)
noisy_moons_tensor = torch.from_numpy(noisy_moons)


train_dataloader = DataLoader(noisy_moons_tensor, batch_size = 100, shuffle=True, num_workers=4)

optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad==True], lr=1e-4)
print(f"Running on: {device}")
net.train()
epoch_count = 5
for t in range(epoch_count):    
  
    # construct the batch
#     noisy_moons = datasets.make_moons(n_samples=100, noise=.05)[0].astype(np.float32)
#     noisy_moons_tensor = torch.from_numpy(noisy_moons).to(device).float()
#     noisy_moons_tensor = noisy_moons_tensor.type(torch.cuda.FloatTensor)

    
    for batch in train_dataloader:      
      optimizer.zero_grad()
      
      loss = -net.log_prob(batch.to(device)).mean() 

      loss.backward(retain_graph=True)
      optimizer.step()
    
    
    print('iter %s:' % t, 'loss = %.3f' % loss)
        
#freeze weights
net.eval()

# draw samples in primary space and map to latent space
plt.subplot(121)
x = datasets.make_moons(n_samples=1000, noise=.05)[0].astype(np.float32)
plt.scatter(x[:, 0], x[:, 1], c='r')
plt.title(r'$X \sim p(X)$')

z = net.f(torch.from_numpy(x).to(device))[0].cpu().detach().numpy()
print(z.shape)
plt.subplot(122)
plt.scatter(z[:, 0], z[:, 1])
plt.title(r'$z = f(X)$')

# draw samples in latent space and map to primary space
# z = np.random.multivariate_normal(np.zeros(2), np.eye(2), 1000)

x, z = net.sample(1000)

x = x.cpu().detach().numpy()
z = z.cpu().detach().squeeze().numpy()

#print(x.shape)
#print(z.shape)

#z = np.random.uniform([-.5, -.5], [.5,.5], (1000, 2))
plt.subplot(121)
plt.scatter(z[:, 0], z[:, 1])
plt.title(r'$z \sim p(z)$')

plt.subplot(122)

plt.scatter(x[:, 0], x[:, 1], c='r')
plt.title(r'$X = g(z)$')